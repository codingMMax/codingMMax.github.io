<!DOCTYPE html>
<!-- 
  Generated by Config-Driven Academic Website Template
  Author: Sixun Dong (ironieser)
  Repository: https://github.com/Ironieser/ironieser.github.io
  License: MIT
-->
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Publications - Sixun Dong</title>
    
    <!-- Favicon -->
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
    <link rel="shortcut icon" href="favicon.ico">
    
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
</head>
<body>
    <!-- Navigation -->
    <header class="header">
        <nav class="nav">
            <div class="nav-container">
                <a href="index.html" class="nav-link " >Bio</a>
                <a href="publications.html" class="nav-link active" >Publications</a>
                <a href="blog.html" class="nav-link " >Blog</a>
                <a href="files/CV_Dong.pdf" class="nav-link " target="_blank">CV(PDF)</a>
            </div>
        </nav>
    </header>

    <!-- Main Content -->
    <main class="main">
        <!-- Page Header -->
        <section class="page-header">
            <div class="container">
                <div class="page-header-content">
                    <h1 class="page-title-left">Publications</h1>
                    <div class="research-intro">
                        <p>Currently, my research focuses on <strong>multimodal AI systems</strong> that bridge computer vision, natural language processing, and machine learning. I work on developing intelligent agents that can understand and generate content across multiple modalities, with applications in video analysis, time series forecasting, and feature transformation.</p>
                    </div>
                    
                    <!-- Summary Stats Bar -->
                    <div class="publication-stats-bar">
                        <span class="stat-item">10+ publications</span> <span class="stat-divider">•</span> <span class="stat-item">4 top-tier venues (CVPR, WACV, IJCAI)</span> <span class="stat-divider">•</span> <span class="stat-item">1 oral presentation</span>
                    </div>
                </div>
            </div>
        </section>

        <!-- Publications -->
        <section class="section">
            <div class="container">
                
            <div class="year-group">
                <h3 class="year-title">2025</h3>
                <div class="publications-list">
                    
                <div class="publication-item">
                    <img src="teaser/underreview.jpg" alt="Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-under-review">Under Review</span> Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives</p>
                        <p class="publication-authors"><span class="author-highlight">Sixun Dong</span>, Wei Fan, Teresa Wu, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2506.24124" target="_blank">Paper</a></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/underreview.jpg" alt="TimesFrame: Multi-Variable Time Series is a Video of Numerical Data" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-under-review">Under Review</span> TimesFrame: Multi-Variable Time Series is a Video of Numerical Data</p>
                        <p class="publication-authors"><span class="author-highlight">Sixun Dong</span>, Nanxu Gong, Haoyue Bai, Xinyuan Wang, Wangyang Ying, Wei Fan, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <span class="coming-soon">Paper (Coming Soon)</span></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/agentic.jpg" alt="Agentic Feature Augmentation: Unifying Selection and Generation with Teaming, Planning, and Memories" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-under-review">arXiv'2505</span> Agentic Feature Augmentation: Unifying Selection and Generation with Teaming, Planning, and Memories</p>
                        <p class="publication-authors">Nanxu Gong*, <span class="author-highlight">Sixun Dong*</span>, Haoyue Bai, Xinyuan Wang, Wangyang Ying, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2505.15076" target="_blank">Paper</a></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/sculp.jpg" alt="Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-under-review">arXiv'2505</span> Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation</p>
                        <p class="publication-authors">Nanxu Gong, Zijun Li, <span class="author-highlight">Sixun Dong</span>, Haoyue Bai, Wangyang Ying, Xinyuan Wang, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2505.15152" target="_blank">Paper</a></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/underreview.jpg" alt="MECT: From Multimodal Knowledge Acquisition To Contrastive Embedding Construction For Generative Feature Transformation" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-under-review">Under Review</span> MECT: From Multimodal Knowledge Acquisition To Contrastive Embedding Construction For Generative Feature Transformation</p>
                        <p class="publication-authors">Nanxu Gong, <span class="author-highlight">Sixun Dong</span>, Haoyue Bai, Wangyang Ying, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <span class="coming-soon">Paper (Coming Soon)</span></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/agentFt.jpg" alt="Unsupervised feature transformation via in-context generation, generator-critic llm agents, and duet-play teaming" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-preprint">arXiv</span> Unsupervised feature transformation via in-context generation, generator-critic llm agents, and duet-play teaming</p>
                        <p class="publication-authors">Nanxu Gong, Xinyuan Wang, Wangyang Ying, Haoyue Bai, <span class="author-highlight">Sixun Dong</span>, Haifeng Chen, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2504.21304" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/NanxuGong/LPFG" target="_blank">Code</a></p>
                    </div>
                </div>
                </div>
            </div>
            <div class="year-group">
                <h3 class="year-title">2024</h3>
                <div class="publications-list">
                    
                <div class="publication-item">
                    <img src="teaser/mmtool.jpg" alt="MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue">WACV</span> MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning</p>
                        <p class="publication-authors">Chenyu Wang, Weixin Luo, <span class="author-highlight">Sixun Dong</span>, Xiaohua Xuan, Zhengxin Li, Lin Ma, Shenghua Gao</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2401.10727" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/MLLM-Tool/MLLM-Tool" target="_blank">Code</a></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/room.jpg" alt="RoomDesigner: Encoding Anchor-latents for Style-consistent and Shape-compatible Indoor Scene Generation" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue">3DV 2024, 2024</span> RoomDesigner: Encoding Anchor-latents for Style-consistent and Shape-compatible Indoor Scene Generation</p>
                        <p class="publication-authors">Yiqun Zhao, Zibo Zhao, Jing Li, <span class="author-highlight">Sixun Dong</span>, Shenghua Gao</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2310.10027" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/zhao-yiqun/RoomDesigner" target="_blank">Code</a></p>
                    </div>
                </div>
                </div>
            </div>
            <div class="year-group">
                <h3 class="year-title">2023</h3>
                <div class="publications-list">
                    
                <div class="publication-item">
                    <img src="teaser/weaksvr.jpg" alt="Weakly Supervised Video Representation Learning with Unaligned Text for Sequential Videos" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue">CVPR</span> Weakly Supervised Video Representation Learning with Unaligned Text for Sequential Videos</p>
                        <p class="publication-authors"><span class="author-highlight">Sixun Dong*</span>, Huazhang Hu*, Dongze Lian, Weixin Luo, Yicheng Qian, Shenghua Gao</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2303.12370" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/svip-lab/WeakSVR" target="_blank">Code</a> / <i class="fab fa-youtube"></i> <a href="https://www.youtube.com/watch?v=AqozSRYP7Pc" target="_blank">YouTube</a> / <i class="fas fa-tv"></i> <a href="https://www.bilibili.com/video/BV1AW4y1R7um" target="_blank">Bilibili</a> / <i class="fas fa-book-open"></i> <a href="https://zhuanlan.zhihu.com/p/617926257" target="_blank">知乎</a></p>
                    </div>
                </div>
                </div>
            </div>
            <div class="year-group">
                <h3 class="year-title">2022</h3>
                <div class="publications-list">
                    
                <div class="publication-item">
                    <img src="teaser/transrac.jpg" alt="TransRAC: Encoding Multi-scale Temporal Correlation with Transformers for Repetitive Action Counting" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue">CVPR</span><span class="publication-venue-oral">🏆 Oral</span> TransRAC: Encoding Multi-scale Temporal Correlation with Transformers for Repetitive Action Counting</p>
                        <p class="publication-authors">Huazhang Hu*, <span class="author-highlight">Sixun Dong*</span>, Yiqun Zhao, Dongze Lian, Zhengxin Li, Shenghua Gao</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2204.01018" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/SvipRepetitionCounting/TransRAC" target="_blank">Code</a> / <i class="fas fa-database"></i> <a href="https://svip-lab.github.io/dataset/RepCount_dataset.html" target="_blank">Dataset</a> / <i class="fab fa-youtube"></i> <a href="https://youtu.be/SFpUS9mHHpk" target="_blank">YouTube</a> / <i class="fas fa-tv"></i> <a href="https://www.bilibili.com/video/BV1B94y1S7oP" target="_blank">Bilibili</a> / <i class="fas fa-book-open"></i> <a href="https://zhuanlan.zhihu.com/p/543376943" target="_blank">知乎</a></p>
                    </div>
                </div>
                </div>
            </div>
            <div class="year-group">
                <h3 class="year-title">Survey Papers</h3>
                <div class="publications-list">
                    
                <div class="publication-item">
                    <img src="teaser/underreview.jpg" alt="Towards Data-Centric AI: A Comprehensive Survey of Traditional, Reinforcement, and Generative Approaches for Tabular Data Transformation" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-under-review">arXiv'2501</span> Towards Data-Centric AI: A Comprehensive Survey of Traditional, Reinforcement, and Generative Approaches for Tabular Data Transformation</p>
                        <p class="publication-authors">Dongjie Wang, Yanyong Huang, Wangyang Ying, Haoyue Bai, Nanxu Gong, Xinyuan Wang, <span class="author-highlight">Sixun Dong</span>, Tao Zhe, Kunpeng Liu, Meng Xiao, et al.</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2501.10555" target="_blank">Paper</a></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/underreview.jpg" alt="A Survey on Data-Centric AI: Tabular Learning from Reinforcement Learning and Generative AI Perspective" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-under-review">arXiv'2502</span> A Survey on Data-Centric AI: Tabular Learning from Reinforcement Learning and Generative AI Perspective</p>
                        <p class="publication-authors">Wangyang Ying, Cong Wei, Nanxu Gong, Xinyuan Wang, Haoyue Bai, Arun Vignesh Malarkkan, <span class="author-highlight">Sixun Dong</span>, Dongjie Wang, Denghui Zhang, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2502.08828" target="_blank">Paper</a></p>
                    </div>
                </div>
                </div>
            </div>
            <div class="year-group">
                <h3 class="year-title">Other Publications <span class="auto-sync-note">Auto-updated based on Google Scholar (Last synced: Jul 21, 2025)</span></h3>
                <div class="publications-list">
                    
                <div class="publication-item">
                    <img src="teaser/preprint.jpg" alt="LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-preprint">arXiv'2506</span> LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation</p>
                        <p class="publication-authors">Xinyuan Wang, Haoyue Bai, Nanxu Gong, Wangyang Ying, <span class="author-highlight">Sixun Dong</span>, X Cui, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=j71Y2-4AAAAJ&sortby=pubdate&citation_for_view=j71Y2-4AAAAJ:_FxGoFyzp5QC" target="_blank">Paper</a></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/preprint.jpg" alt="Efficient Post-Training Refinement of Latent Reasoning in Large Language Models" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-preprint">arXiv'2506</span> Efficient Post-Training Refinement of Latent Reasoning in Large Language Models</p>
                        <p class="publication-authors">Xinyuan Wang, Dongjie Wang, Wangyang Ying, Haoyue Bai, Nanxu Gong, <span class="author-highlight">Sixun Dong</span>, Kunpeng Liu, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=j71Y2-4AAAAJ&sortby=pubdate&citation_for_view=j71Y2-4AAAAJ:ufrVoPGSRksC" target="_blank">Paper</a></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/preprint.jpg" alt="Brownian Bridge Augmented Surrogate Simulation and Injection Planning for Geological CO Storage" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-preprint">arXiv'2505</span> Brownian Bridge Augmented Surrogate Simulation and Injection Planning for Geological CO Storage</p>
                        <p class="publication-authors">Haoyue Bai, G Chen, Wangyang Ying, Xinyuan Wang, Nanxu Gong, <span class="author-highlight">Sixun Dong</span>, G Pedrielli, H Wang, ...</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=j71Y2-4AAAAJ&sortby=pubdate&citation_for_view=j71Y2-4AAAAJ:WF5omc3nYNoC" target="_blank">Paper</a></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/preprint.jpg" alt="Bridging the domain gap in equation distillation with reinforcement feedback" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-preprint">arXiv'2505</span> Bridging the domain gap in equation distillation with reinforcement feedback</p>
                        <p class="publication-authors">Wangyang Ying, Haoyue Bai, Nanxu Gong, Xinyuan Wang, <span class="author-highlight">Sixun Dong</span>, H Chen, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=j71Y2-4AAAAJ&sortby=pubdate&citation_for_view=j71Y2-4AAAAJ:eQOLeE2rZwMC" target="_blank">Paper</a></p>
                    </div>
                </div>
                </div>
            </div>
            </div>
        </section>
    </main>

    
    <footer class="footer">
        <div class="container">
            <!-- Visitor Map Section -->
            <div class="visitor-map-section">
                <div class="visitor-map-container">
                    <!-- Visitor Map Widget -->
                    <div class="visitor-map">
                        <!-- ClustrMaps Widget -->
                        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=r_cMMykDPAdqK2GTahWbR__mtnzcj9svUgejZ86OXnU&cl=ffffff&w=a"></script>
                    </div>
                </div>
            </div>

            <div class="footer-stats">
                <div class="stats-item">
                    <i class="fas fa-map-marker-alt"></i>
                    Last updated from: <span id="owner-location">Loading...</span>
                </div>
                <div class="stats-item">
                    <i class="fas fa-clock"></i>
                    Content last updated: <span id="last-updated"></span>
                </div>
            </div>
            
            <div class="template-credit">
                <p>Built with <a href="https://github.com/Ironieser/ironieser.github.io" target="_blank" rel="noopener">Config-Driven Academic Website Template</a> by <a href="https://github.com/Ironieser/ironieser.github.io" target="_blank" rel="noopener">Sixun Dong (ironieser)</a></p>
                <p class="template-acknowledgments">Built with the help of Cursor IDE, Claude AI, and GPT - enabling non-experts to create personalized academic websites through modern AI tools</p>
            </div>
            <p>&copy; 2025 Sixun Dong. All rights reserved.</p>
        </div>
    </footer>
    
    
    <script>
        // Get website owner's location during build (not visitor's location)
        async function getOwnerLocation() {
            try {
                // Try primary API first
                let response = await fetch('https://ipapi.co/json/');
                let data = await response.json();
                
                if (data.city && data.country_name) {
                    const location = `${data.city}, ${data.country_name}`;
                    document.getElementById('owner-location').textContent = location;
                    return;
                }
                
                // If primary API fails, try backup API
                response = await fetch('https://api.ipify.org?format=json');
                const ipData = await response.json();
                
                if (ipData.ip) {
                    // Use a different geolocation service
                    response = await fetch(`https://ip-api.com/json/${ipData.ip}`);
                    data = await response.json();
                    
                    if (data.city && data.country) {
                        const location = `${data.city}, ${data.country}`
                        document.getElementById('owner-location').textContent = location;
                        return;
                    }
                }
                
                // If all APIs fail, show a default message
                document.getElementById('owner-location').textContent = 'Remote Server';
                
            } catch (error) {
                console.log('Location detection failed:', error);
                // For GitHub Actions builds, show a more appropriate message
                document.getElementById('owner-location').textContent = 'GitHub Actions';
            }
        }
        
        // Set last updated time (when GitHub Actions built the site)
        function setLastUpdated() {
            const buildDate = '2025-07-21';
            const date = new Date(buildDate + 'T12:00:00'); // Add noon time to avoid timezone issues
            const formatted = date.toLocaleDateString('en-US', { 
                year: 'numeric', 
                month: 'short', 
                day: 'numeric' 
            });
            document.getElementById('last-updated').textContent = formatted;
        }
        
        // Initialize on page load
        document.addEventListener('DOMContentLoaded', function() {
            getOwnerLocation();
            setLastUpdated();
        });
    </script>
</body>
</html>